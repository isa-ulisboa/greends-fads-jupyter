{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d442f150",
   "metadata": {},
   "source": [
    "### GreenDS\n",
    "\n",
    "# Fundamentals of Agro-Environmental Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26b0967",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "The purpose of this Jupyter Notebook exercise is to demonstrate the workflow in a data science project using this IDE. We will use an example on healthcare that is very much publicised in the data science community.\n",
    "\n",
    "It is not expected that you can understand the code that is being executed, or the algorithms applied. You will be able to do it by the end of this first master's year. But you should be able to identify, at the high level, the different steps on the data science process:\n",
    "- Formulate the question\n",
    "- Obtain and prepare data\n",
    "- Perform data exploration and analysis\n",
    "- Develop the model\n",
    "- Present the results\n",
    "\n",
    "Another goal is to understand the interface of Jupyter Notebook, how to run code and render markdown cells, and solve problems you may find with unexisting libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e1dd97",
   "metadata": {},
   "source": [
    "# Examples of Data Science Use Cases\n",
    "\n",
    "## Data Science in Healthcare - Predicting Breats Cancer\n",
    "\n",
    "This example is based on the following Datacamp [Use Case](https://www.datacamp.com/blog/data-science-use-cases-guide) and Kaggle [example](https://www.kaggle.com/code/vincentlugat/breast-cancer-analysis-and-prediction).\n",
    "\n",
    "The use case shows an exercise of predition of breast cancer. The dataset is based on measurements made on digitized images of biopsies (fine needle aspiration) of a breast mass. The attributes are as described in: [K. P. Bennett and O. L. Mangasarian: \"Robust Linear Programming Discrimination of Two Linearly Inseparable Sets\", Optimization Methods and Software 1, 1992, 23-34]\n",
    "\n",
    "Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. In the dataset, each case is classified as being benign or malignant, which is classified in the Diagnosis attribute. \n",
    "\n",
    "- ID number\n",
    "- Diagnosis (M = malignant, B = benign)\n",
    "\n",
    "Ten real-valued features are computed for each cell nucleus:\n",
    "\n",
    "- radius (mean of distances from center to points on the perimeter)\n",
    "- texture (standard deviation of gray-scale values)\n",
    "- perimeter\n",
    "- area\n",
    "- smoothness (local variation in radius lengths)\n",
    "- compactness (perimeter^2 / area - 1.0)\n",
    "- concavity (severity of concave portions of the contour)\n",
    "- concave points (number of concave portions of the contour)\n",
    "- symmetry\n",
    "- fractal dimension (\"coastline approximation\" - 1)\n",
    "\n",
    "The mean, standard error, and worst or largets (mean of the three largest of these features were computed for each image, resulting in 30 features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ca1cb8",
   "metadata": {},
   "source": [
    "## 1. Prepare your environment:\n",
    "- Create a `raw-data`directory in your project's directory to place external data files.\n",
    "- install and load python libraries necessary to run the python code. This project requires the following libraries:\n",
    "  - pandas\n",
    "  - sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39acd925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't have pandas library installed, you can do it at the shell terminal\n",
    "# with the following commands:\n",
    "#\n",
    "# $ pip3 install pandas\n",
    "# $ pip3 install sklearn\n",
    "# $ pip3 install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fd6f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas library\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e45c1d",
   "metadata": {},
   "source": [
    "## 2. Download the data file from Kaggle\n",
    "Go to http://www.kaggle.com/uciml/breast-cancer-wisconsin-data and download the `data.csv` file. Place the file at the `raw-data` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff89ade",
   "metadata": {},
   "source": [
    "## 3. Read and preview data\n",
    "Read the data file, and print the shape and a preview of the table:\n",
    "- number of rows\n",
    "- number of properties (columns)\n",
    "- show the first two rows of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70285455",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_data = pd.read_csv('./raw-data/data.csv')\n",
    "pd.options.display.max_columns = len(cancer_data)\n",
    "print(f'Number of entries: {cancer_data.shape[0]:,}\\n'\n",
    "      f'Number of features: {cancer_data.shape[1]:,}\\n\\n'\n",
    "      f'Number of missing values: {cancer_data.isnull().sum().sum()}\\n\\n'\n",
    "      f'{cancer_data.head(2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f30a4d6",
   "metadata": {},
   "source": [
    "The table was loaded to `pandas`, which has the possibility to show a preview of data (head):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4aa62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14552508",
   "metadata": {},
   "source": [
    "## 4. Clean and explore data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2a2100",
   "metadata": {},
   "source": [
    "You can scroll to the last column of the table above and verify that it contains no values (NaN). We need to remove the last column with missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625e3680",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_data = cancer_data.drop('Unnamed: 32', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ea82b6",
   "metadata": {},
   "source": [
    "It is possible to calculate descriptive statistics parameters of the attributes of the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21fea3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dd3b60",
   "metadata": {},
   "source": [
    "Next, let's calculate how many women have a confirmed cancer (a malignant breast tumor)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8177bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_data['diagnosis'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef669182",
   "metadata": {},
   "source": [
    "We can calculate these values as percentages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfcb2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(cancer_data['diagnosis'].value_counts()*100/len(cancer_data)).convert_dtypes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1acac8",
   "metadata": {},
   "source": [
    "## 5. Visualize data\n",
    "\n",
    "We can get a better insight of the data if we compare values for benign and malignant cases. Seaborn is one of the powerfull libraries to visualize data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c1f1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set(style=\"ticks\", color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384de985",
   "metadata": {},
   "outputs": [],
   "source": [
    "radius = cancer_data[['radius_mean','radius_se','radius_worst','diagnosis']]\n",
    "sns.pairplot(radius, hue='diagnosis',palette=\"husl\", markers=[\"o\", \"s\"],height=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03462eb",
   "metadata": {},
   "source": [
    "We can do another visualization, adding linear regression lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f03a0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "texture = cancer_data[['texture_mean','texture_se','texture_worst','diagnosis']]\n",
    "sns.pairplot(texture, hue='diagnosis', palette=\"husl\",height=4, kind=\"reg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a6799e",
   "metadata": {},
   "source": [
    "Another visualization which display the histogram for each category, is called violinplot. We will do this in groups of ten variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557ba6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y includes our labels and x includes our features\n",
    "y = cancer_data.diagnosis # M or B \n",
    "list_drp = ['id','diagnosis']\n",
    "x = cancer_data.drop(list_drp, axis = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bd95e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_dia = y\n",
    "data = x\n",
    "# standardization of the data\n",
    "data_n_2 = (data - data.mean()) / (data.std())\n",
    "data = pd.concat([y,data_n_2.iloc[:,0:30]],axis=1)\n",
    "data = pd.melt(data,id_vars=\"diagnosis\",\n",
    "                    var_name=\"features\",\n",
    "                    value_name='value')\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data,split=True, inner=\"quart\",palette ={\"B\": \"g\", \"M\": \"r\"})\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75426d3",
   "metadata": {},
   "source": [
    "We can represent using box plots the worst values of the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568c810c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# box-plots\n",
    "data = pd.concat([y,data_n_2.iloc[:,20:30]],axis=1)\n",
    "data = pd.melt(data,id_vars=\"diagnosis\",\n",
    "                    var_name=\"features\",\n",
    "                    value_name='value')\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.boxplot(x='features', y='value', hue='diagnosis', data=data, palette ={\"B\": \"g\", \"M\": \"r\"})\n",
    "plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0afa27",
   "metadata": {},
   "source": [
    "To explore correlations between independent variables, we can calculate the correlation matrix, and represented with a heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6070ab1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation map\n",
    "f,ax = plt.subplots(figsize=(18, 18))\n",
    "sns.heatmap(x.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6924b714",
   "metadata": {},
   "source": [
    "## 6. Build the model\n",
    "\n",
    "We will calculate two models: on based on the [K-nearest neighors (KNN)](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) algorithm, and the other based on [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression).\n",
    "\n",
    "First, we will define the X (independent) and Y (dependent) variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4036bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cancer_data.iloc[:, 2:32].values\n",
    "y = cancer_data.iloc[:, 1].values\n",
    "\n",
    "# Encoding categorical data\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "labelencoder_y = LabelEncoder()\n",
    "y = labelencoder_y.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6152fd0",
   "metadata": {},
   "source": [
    "It is important to divide the dataset in two subsets, one for training (creating) the model, and other for testing. This is important to make sure that the model is not overfitted, and can be applied to other data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a43b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b525799a",
   "metadata": {},
   "source": [
    "In this example, we will try two models:\n",
    "- K-Nearest Neighbor (KNN)\n",
    "- Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3319c0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# KNN\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "knn_predictions = knn.predict(X_test)\n",
    "\n",
    "# Logistic regression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr_predictions = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bc4d05",
   "metadata": {},
   "source": [
    "## 7. Evaluate the model\n",
    "\n",
    "We can calculate the accuracy of the models. This value returns the fraction of correctly classified samples, in the test subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75564eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "print(f'Accuracy scores:\\n'\n",
    "      f'KNN model:\\t\\t   {accuracy_score(y_test, knn_predictions):.3f}\\n'\n",
    "      f'Logistic regression model: {accuracy_score(y_test, lr_predictions):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa3a6ab",
   "metadata": {},
   "source": [
    "Another way of evaluation the model is to calculate the confusion matrix *C*, in which *C<sub>i,j</sub>* is the number of observations which true value is *i*, and was predicted to be *j*.\n",
    "It gives the values of true negatives (*C<sub>0,0</sub>*), false negatives (*C<sub>1,0</sub>*), true positives (*C<sub>1,1</sub>*) and false positives (*C<sub>0,1</sub>*). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574cf8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = confusion_matrix(knn_predictions, y_test)\n",
    "sns.heatmap(matrix, cbar=False, annot=True)\n",
    "plt.xlabel('Predict')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix - Logistic Regression model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
