{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d442f150",
   "metadata": {},
   "source": [
    "### GreenDS\n",
    "\n",
    "# Fundamentals of Agro-Environmental Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94780fb",
   "metadata": {},
   "source": [
    "## Example APIs and Web scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26b0967",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "The purpose of this Jupyter Notebook exercise is to demonstrate the methods available to obtain data from online services. Two examples are explored:\n",
    "- web data services based on REST APIs\n",
    "- web scraping from online web pages\n",
    "\n",
    "Sometime, web pages use APIs to expose information and services, but no documentation is provided. We will learn how to identify the existence of these services, to use them in more efficient data collection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc00653",
   "metadata": {},
   "source": [
    "## 1. Web scraping list of TOP TV SHOWS\n",
    "\n",
    "This was the example run in class. It is based on the Jupyter Notebook provided \n",
    "by Mrityunjay Pathak in [Kaggle](https://www.kaggle.com/discussions/general/527927). \n",
    "The goal is to create a list of IMDB Top 250 TV Shows by web scrapping. The code \n",
    "is made available [as a Jupyter Notebook](https://github.com/TheMrityunjayPathak/Data-Science-with-Python/blob/main/Web%20Scrapping/Web%20Scrapping%20with%20Python.ipynb) in Github."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041b5644",
   "metadata": {},
   "source": [
    "First, we need to import the following libraries:\n",
    "\n",
    "**pandas**\n",
    "- This library is used for data manipulation and analysis.\n",
    "- It provides powerful data structures like DataFrames which are great for organizing and analyzing data scraped from websites.\n",
    "- For example, after scraping data you can use pandas to clean and save the data in various formats like CSV or Excel.\n",
    "\n",
    "**requests**\n",
    "- This library allows you to send HTTP requests using Python.\n",
    "- It's often used to fetch the HTML content of a webpage.\n",
    "- With requests, you can easily retrieve the page source which can then be parsed to extract the desired information.\n",
    "\n",
    "**BeautifulSoup**\n",
    "- This library is used for parsing HTML and XML documents.\n",
    "- It makes it easy to navigate and search the HTML structure of a webpage.\n",
    "- After retrieving the HTML content using requests you can use BeautifulSoup to parse and extract specific elements of the webpage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7f821fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "# requests\n",
    "import requests\n",
    "# BeautifulSoup\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7236c7",
   "metadata": {},
   "source": [
    "Now, let's obtain the source html code of the web page with the list we want to obtain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "004e5ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en-US\" xmlns:fb=\"http://www.facebook.com/2008/fbml\" xmlns:og=\"http://opengraphprotocol.org/schema/\">\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <meta content=\"width=device-width\" name=\"viewport\"/>\n",
      "  <script>\n",
      "   if(typeof uet === 'function'){ uet('bb', 'LoadTitle', {wb: 1}); }\n",
      "  </script>\n",
      "  <script>\n",
      "   window.addEventListener('load', (event) => {\n",
      "        if (typeof window.csa !== 'undefined' && typeof window.csa === 'function') {\n",
      "            var csaLatencyPlugin = window.csa(\n"
     ]
    }
   ],
   "source": [
    "# URL of IMDb's Top TV page\n",
    "url = \"https://www.imdb.com/chart/toptv/\"\n",
    "\n",
    "# Set headers to mimic a real browser visit\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Send GET request to the webpage\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Get page content\n",
    "    page_source = response.text\n",
    "    # Parse with BeautifulSoup\n",
    "    soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "    # Display or process the parsed page\n",
    "    print(soup.prettify()[:500])\n",
    "else:\n",
    "    print(f\"Failed to retrieve page. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c7a2ca",
   "metadata": {},
   "source": [
    "Using BeautifullSoup, we can extract the main title of the base, which is formatted \n",
    "in HTML with the tag `<h1>`. We will print the result. This corresponds to this part of the webpage.\n",
    "\n",
    "Using the Inspector tool in your browser you can confirm that this is a heading **h1**.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/TheMrityunjayPathak/Data-Science-with-Python/main/Web%20Scrapping/images/heading.png\" width=\"500px\" border=\"2px solid black\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef468a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 250 TV Shows\n"
     ]
    }
   ],
   "source": [
    "# Heading of the Webpage\n",
    "heading = soup.find(\"h1\").text\n",
    "print(heading)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943860a1",
   "metadata": {},
   "source": [
    "Now, let's obtain the TV Show names.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/TheMrityunjayPathak/Data-Science-with-Python/main/Web%20Scrapping/images/tv%20show%20name.png\" width=\"500px\" border=\"2px solid black\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f336144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IMDb Charts', '1. Breaking Bad: Ruptura Total', '2. Planet Earth II', '3. Planet Earth', '4. Irmãos de Armas', '5. Chernobyl', '6. The Wire', '7. Avatar: O Último Airbender', '8. Blue Planet II', '9. Os Sopranos', '10. Cosmos: A Spacetime Odyssey', '11. Cosmos', '12. Our Planet', '13. A Guerra dos Tronos', '14. Bluey', '15. The World at War', '16. Hagane no renkinjutsushi', '17. Rick e Morty', '18. Life - Vida Selvagem', '19. The Last Dance', '20. No Limiar da Realidade', '21. The Vietnam War', '22. Sherlock', '23. Ataque dos Titãs', '24. Batman: A Série Animada', '25. O Escritório', 'Recently viewed']\n"
     ]
    }
   ],
   "source": [
    "# TV Shows Name\n",
    "lst1 = []\n",
    "for i in soup.find_all(\"h3\",\"ipc-title__text\"):\n",
    "    lst1.append(i.text)\n",
    "\n",
    "print(lst1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77ea7125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1. Breaking Bad: Ruptura Total', '2. Planet Earth II', '3. Planet Earth', '4. Irmãos de Armas', '5. Chernobyl', '6. The Wire', '7. Avatar: O Último Airbender', '8. Blue Planet II', '9. Os Sopranos', '10. Cosmos: A Spacetime Odyssey', '11. Cosmos', '12. Our Planet', '13. A Guerra dos Tronos', '14. Bluey', '15. The World at War', '16. Hagane no renkinjutsushi', '17. Rick e Morty', '18. Life - Vida Selvagem', '19. The Last Dance', '20. No Limiar da Realidade', '21. The Vietnam War', '22. Sherlock', '23. Ataque dos Titãs', '24. Batman: A Série Animada', '25. O Escritório', 'Recently viewed']\n"
     ]
    }
   ],
   "source": [
    "# To remove additional <h3> Text from TV Shows Name\n",
    "lst1 = lst1[1:251]\n",
    "print(lst1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d914d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping item 'Recently viewed' as it doesn't contain a period.\n",
      "['Breaking Bad: Ruptura Total', 'Planet Earth II', 'Planet Earth', 'Irmãos de Armas', 'Chernobyl', 'The Wire', 'Avatar: O Último Airbender', 'Blue Planet II', 'Os Sopranos', 'Cosmos: A Spacetime Odyssey', 'Cosmos', 'Our Planet', 'A Guerra dos Tronos', 'Bluey', 'The World at War', 'Hagane no renkinjutsushi', 'Rick e Morty', 'Life - Vida Selvagem', 'The Last Dance', 'No Limiar da Realidade', 'The Vietnam War', 'Sherlock', 'Ataque dos Titãs', 'Batman: A Série Animada', 'O Escritório']\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store names\n",
    "name_list = []\n",
    "\n",
    "# Loop through each item in the list\n",
    "for i in lst1:\n",
    "    parts = i.split(\".\")\n",
    "    if len(parts) > 1:  # Ensure there are at least two parts after splitting\n",
    "        name_list.append(parts[1].strip())  # Append the second part after stripping extra whitespace\n",
    "    else:\n",
    "        print(f\"Skipping item '{i}' as it doesn't contain a period.\")\n",
    "\n",
    "print(name_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e25303",
   "metadata": {},
   "source": [
    "We can also identify and obtain the rating of the TV shows. It is only necessary \n",
    "with the inspector identify which html tag and css class is used in the webpage.\n",
    "\n",
    "It is `span` and `ipc-rating-star--rating`, that we can use to identify the values\n",
    "in BeautifulSoup.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/TheMrityunjayPathak/Data-Science-with-Python/main/Web%20Scrapping/images/ratings.png\" width=\"500px\" border=\"2px solid black\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cab64c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['9.5', '9.5', '9.4', '9.4', '9.3', '9.3', '9.3', '9.3', '9.2', '9.2', '9.3', '9.2', '9.2', '9.3', '9.2', '9.1', '9.1', '9.1', '9.1', '9.0', '9.1', '9.1', '9.1', '9.0', '9.0']\n"
     ]
    }
   ],
   "source": [
    "# IMDB Rating\n",
    "rating = []\n",
    "for i in soup.find_all(\"span\",\"ipc-rating-star--rating\"):\n",
    "    rating.append(i.text.strip())\n",
    "\n",
    "print(rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a7931a",
   "metadata": {},
   "source": [
    "We can do the same o get the number of votes.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/TheMrityunjayPathak/Data-Science-with-Python/main/Web%20Scrapping/images/votes.png\" width=\"500px\" border=\"2px solid black\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0bbfb776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2.2M', '162K', '223K', '544K', '905K', '390K', '388K', '48K', '497K', '131K', '45K', '53K', '2.4M', '33K', '31K', '208K', '626K', '43K', '159K', '96K', '29K', '1M', '559K', '122K', '744K']\n"
     ]
    }
   ],
   "source": [
    "# Aggregated loop for extracting and cleaning votes\n",
    "votes = []\n",
    "for i in soup.find_all(\"span\", \"ipc-rating-star--voteCount\"):\n",
    "    votes.append(i.text.strip().replace(\"(\", \"\").replace(\")\", \"\").strip())\n",
    "\n",
    "print(votes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3784b341",
   "metadata": {},
   "source": [
    "So, we have created three python lists:\n",
    "- name of the show\n",
    "- rating\n",
    "- number of votes\n",
    "\n",
    "Let's assemble everything in a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "526bfbd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Breaking Bad: Ruptura Total</td>\n",
       "      <td>9.5</td>\n",
       "      <td>2.2M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Planet Earth II</td>\n",
       "      <td>9.5</td>\n",
       "      <td>162K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Planet Earth</td>\n",
       "      <td>9.4</td>\n",
       "      <td>223K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Irmãos de Armas</td>\n",
       "      <td>9.4</td>\n",
       "      <td>544K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chernobyl</td>\n",
       "      <td>9.3</td>\n",
       "      <td>905K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Wire</td>\n",
       "      <td>9.3</td>\n",
       "      <td>390K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Avatar: O Último Airbender</td>\n",
       "      <td>9.3</td>\n",
       "      <td>388K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Blue Planet II</td>\n",
       "      <td>9.3</td>\n",
       "      <td>48K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Os Sopranos</td>\n",
       "      <td>9.2</td>\n",
       "      <td>497K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Cosmos: A Spacetime Odyssey</td>\n",
       "      <td>9.2</td>\n",
       "      <td>131K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Name Ratings Votes\n",
       "0  Breaking Bad: Ruptura Total     9.5  2.2M\n",
       "1              Planet Earth II     9.5  162K\n",
       "2                 Planet Earth     9.4  223K\n",
       "3              Irmãos de Armas     9.4  544K\n",
       "4                    Chernobyl     9.3  905K\n",
       "5                     The Wire     9.3  390K\n",
       "6   Avatar: O Último Airbender     9.3  388K\n",
       "7               Blue Planet II     9.3   48K\n",
       "8                  Os Sopranos     9.2  497K\n",
       "9  Cosmos: A Spacetime Odyssey     9.2  131K"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collecting all the Data into a DataFrame\n",
    "tv_shows = pd.DataFrame({\"Name\":name_list,\"Ratings\":rating,\"Votes\":votes})\n",
    "tv_shows.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fbe196",
   "metadata": {},
   "source": [
    "And we can export the full list to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "502c03f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the DataFrame as CSV File\n",
    "tv_shows.to_csv(\"tv_shows.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e1dd97",
   "metadata": {},
   "source": [
    "## 2. Web scraping Air Quality data\n",
    "\n",
    "The QUALAR (https://qualar.apambiente.pt/) is a web platform of APA, the Portuguese Environment Agency, that displays online air quality data sampled by on air monitoring stations in Portugal. Unfortunately, the platform does not expose to the final user, or provides documentation how to use the API service that was implemented for web users downloads. Downloads are generated as XLSX files.\n",
    "\n",
    "However, it is possible to hack the source code of the webpage to identify that there is an implemented API, and that it can be used to facilite efficient download of data. This exercise will demonstrate that, with the following steps:\n",
    "- check how are downloads generated from the website\n",
    "- identify and collect the parameters that define the data download\n",
    "- use the API to download data\n",
    "- as a bonus, visualise a timeseries of a data quality variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c965a4da",
   "metadata": {},
   "source": [
    "## Prepare your environment\n",
    "\n",
    "1. Create two directories inside your projects' directory named `raw-data` and `qualar-data`, with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "403626d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘raw-data’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir raw-data qualar-data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ca1cb8",
   "metadata": {},
   "source": [
    "## 1. Data download for human users\n",
    "\n",
    "Visit the data download page of QUALAR, at https://qualar.apambiente.pt/downloads. It displays a table with a list of Air Monitoring Stations, with the following columns:\n",
    "- Region (Região)\n",
    "- Municipality (Concelho)\n",
    "- Station (Estação)\n",
    "- Station type (Tipo de Estação), with categories traffic, industrial and background\n",
    "- Área type (Tipo de Área), with categories urban, suburban and rural\n",
    "- columns for the following pollutants: O3, NO2, CO, SO2, PM10, PM2.5, C6H6, other\n",
    "\n",
    "On the top, the page has two fields to define the time range for the data download, and on the left several buttons to activate filters about the type of station and type of área.\n",
    "\n",
    "To make a download, users can click on arrows that are available for each station and each pollutant, or if they want to download all pollutants for a station, they can click directly on the station name. After clicking, the download file is generated for the requested options as a excel file (xlsx).\n",
    "\n",
    "*Try to make a download in this way, and check the file downloaded.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5ced8c",
   "metadata": {},
   "source": [
    "## 2. Verify how are downloads generated by inspecting the webpage source code\n",
    "\n",
    "It is possible to inspect the source code of the table, and the behaviour of the page when a download is solicited. Checking this we can try to identify which methods are used to provide data to users. If we manage to verify that the web page is served by an API, and we can identify which parameters define a request, then it would be possible to generate a script to speed up downloads.\n",
    "\n",
    "**1. Activate the Inspect Tool of the source code of the webpage.**\n",
    "\n",
    "*Open in your web browser, navigate to https://qualar.apambiente.pt/downloads. Afterwards, in the menu of your browser, find the option **Web Developer Tools** or **Developer Tools** (in Firefox or Chrome, you will find it in **More tools**). This will open a new panel in the browser.*\n",
    "\n",
    "**2. Check the method to generate downloads**\n",
    "\n",
    "As mentioned before, clicking on the name of a station will generate a download with all data for that station. This means that through the HTTP protocol, a request is made through the network. Checking which request was made (which URL request was send) is a good way of verifying what was the information send to the web server.\n",
    "\n",
    "*On the Developer Panel, click on the tab **Netwotk**. After that, click on the name of a station to make a download request. This will generate a new row on the panel, with the information about a **GET** request.*\n",
    "\n",
    "One of the parameters in that row is the name or file field, which shows the URL sent to the server, e.g.:\n",
    "\n",
    "```https://qualar.apambiente.pt/api/download.php?poluente_id=0&estacao_id=3082&data_inicio=2021-01-01&data_fim=2021-12-31&influencias=1,2,3&ambientes=1,2,4```\n",
    "\n",
    "We can identify the following sections in the URL:\n",
    "\n",
    "Host URL: ```https://qualar.apambiente.pt/api/download.php```\n",
    "\n",
    "Parameters:```poluente_id=0&estacao_id=3082&data_inicio=2021-01-01&data_fim=2021-12-31&influencias=1,2,3&ambientes=1,2,4```\n",
    "\n",
    "The meaning of the parameters is more or less obvious:\n",
    "\n",
    "`poluente_id` - the ID of the pollutant. The value zero should mean all pollutants\n",
    "\n",
    "`estacao_ID` - the ID of the station\n",
    "\n",
    "`data_inicio` - starting date\n",
    "\n",
    "`data_fim` - ending date\n",
    "\n",
    "`influencias` - station type\n",
    "\n",
    "`ambientes` - area type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be9a9d6",
   "metadata": {},
   "source": [
    "**3. Verify that the method for download works**\n",
    "\n",
    "We have just identified an API service for downloading data. We can check if it works, testing with different parameters and see if results correspond to what is expected:\n",
    "\n",
    "*To download data only for **year 2020**, try the following modified URL:*\n",
    "\n",
    "```https://qualar.apambiente.pt/api/download.php?poluente_id=0&estacao_id=3082&data_inicio=2020-01-01&data_fim=2020-12-31&influencias=1,2,3&ambientes=1,2,4```\n",
    "\n",
    "The challenge now is to discover the values of the IDs of the air monitoring stations (the parameter `estacao_ID`). If we find these, we can make a script to make automatic requests to download the data files. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9a47c0",
   "metadata": {},
   "source": [
    "**4. Inspect the HTML source code** \n",
    "\n",
    "On the top left bar of the **Developer tools** panel, there is a arrow cursor option. Select this, and the place the mouse pointed on the name of one air monitoring station, in the table. You will verify that for each section of the web page where you hover your mouse, the corresponding HTML source code will be highlighted in the developer panel.\n",
    "\n",
    "**5. Select the section of the HTML code with the cell of the station name**\n",
    "\n",
    "Place the mouse so that the complete cell with the name of a station in the table is highlighted, and the click. In the source code, a line starting with the tag **td** should be selected.\n",
    "\n",
    "In the begining of that line, a triangle indicates that the inner HTLM code can be expanded. Remember that HTML is a hierarchical language, where html tags placed inside other tags are \"child\" or \"depended\" of these.\n",
    "\n",
    "The html line looks, for example, like the following:\n",
    "\n",
    "```html\n",
    "<td style=\"background-color: #EBF7FF; text-align: center; vertical-align: middle; \">\n",
    "    <label title=\"Dados de todos os poluentes para uma estação num dado ano\" \n",
    "           style=\"color:#0000ff; cursor:pointer\" onclick=\"tableDataManager.openExcel(3082)\">\n",
    "        <u><b>Alfragide/Amadora</b></u>\n",
    "    </label>\n",
    "</td>\n",
    "```\n",
    "\n",
    "The interesting about that html is that the **onclick** event on the **label** tag is an actionable event that triggers a method to open an Excel, **with an ID 3082**. This was for the station Alfragide/Amadora. If we try another station, the ID will be different. We have, therefore, found a way of identifying the IDs of all air monitoring stations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f5d443",
   "metadata": {},
   "source": [
    "## 3. Scrap the HTML source code to obtain IDs of the stations\n",
    "\n",
    "We will scrap the html of table in https://qualar.apambiente.pt/downloads to obtain the IDs of the air monitoring stations. For that, we will use the phyton module **BeautifulSoup**.\n",
    "\n",
    "**1. Get the table content**\n",
    "\n",
    "In the web page, the table content is loaded dynamically. It is possible to use python libraries to obtain that html code, but it is easier to do it manually.\n",
    "- go to the Inspector or Elements panel of the Developer Tool in your browser, depending of which browser you use\n",
    "- navigate down through the html code hierarchy until you find the html tag \\<tbody\\>. Verify in the web page that the correct section of the table containing data is highlighted.\n",
    "- right-click on top of the \\<tbody\\> tag, and select Copy --> Copy Outer HTML\n",
    "- paste the memory context to a new file in your text editor (VS Code, Notepad++, etc), and save it with the name *qualar_table.html* in the `raw-data` o your project.\n",
    "\n",
    "**2. Install and import** *BeautifulSoup*\n",
    "\n",
    "Prepare your system for BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39acd925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't have BeautifulSoup library installed, you can do it at the shell terminal\n",
    "# with the following commands:\n",
    "#\n",
    "# $ pip3 install BeautifulSoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af1829f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import BeautifulSoup and open the html file\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d8b9d0",
   "metadata": {},
   "source": [
    "**3. Get air pollution station IDs**\n",
    "\n",
    "Extract the station ID from the html text using BeautifulSoup. We will store the list of station IDs in a list to use latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d7d58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the html file\n",
    "with open(\"./raw-data/qualar_table.html\") as html_table:\n",
    "    soup = BeautifulSoup(html_table, 'html.parser')\n",
    "\n",
    "# create a list to store ids\n",
    "station_ids = []\n",
    "\n",
    "\n",
    "# get ids from html\n",
    "for row in soup.find_all('label'):\n",
    "    row_text = row['onclick']\n",
    "    ids = row_text[row_text.find( '(' )+1:row_text.find( ')' )]\n",
    "    station_ids.append(ids)\n",
    "    \n",
    "print(station_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2041e67b",
   "metadata": {},
   "source": [
    "## 4. Download files using the API\n",
    "\n",
    "We can now make requests to the API to download xlsx files. First, let's import the python modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fd6f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import requests\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8aefe4",
   "metadata": {},
   "source": [
    "After that, we will request to the REST API the excel files for each stattion, identified by its ID, and store them localy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8ebdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base URL of the service\n",
    "base_url = 'https://qualar.apambiente.pt/api/download.php'\n",
    "\n",
    "# Define params for the API request\n",
    "params = {\"poluente_id\":\"0\", \"influencias\":\"1,2,3\", \"ambientes\":\"1,2,4\"}\n",
    "\n",
    "# Define begining of the data period. You can change this for another year\n",
    "params[\"data_inicio\"] = \"2021-01-01\"\n",
    "\n",
    "# Define end of the data period. You can change this for another year\n",
    "params[\"data_fim\"] = \"2021-12-31\"\n",
    "\n",
    "# As this is not in production, we will replace the full list of ids by a smaller list of ids, to save time. \n",
    "# If you want all data, comment the next line\n",
    "station_ids = ['3095', '3103', '3104', '3096', '3070']\n",
    "\n",
    "# This iteration will go through the list of IDs, request the excel file based on the URL and save it locally.\n",
    "for id_value in station_ids:\n",
    "    # define name of file to be saved\n",
    "    filename = './qualar-data/station_'+ id_value + '.xlsx'\n",
    "    # add station_id to the parameters of the URL\n",
    "    params[\"estacao_id\"] = id_value\n",
    "    try:\n",
    "        r = requests.get(base_url, params)\n",
    "    except requests.exceptions.RequestException as e: \n",
    "        raise SystemExit(e)\n",
    "    print(r.url)\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd499dd",
   "metadata": {},
   "source": [
    "## 5. Preview dowloaded data\n",
    "\n",
    "We can import xlsx data to Pandas, and make a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a67bfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0421480",
   "metadata": {},
   "source": [
    "Let's print the list of available excel files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e45e7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list xlsx files in the data directory\n",
    "\n",
    "files = os.listdir('./qualar-data')\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b5e74e",
   "metadata": {},
   "source": [
    "The following imports the files to a Pandas dataframe, and creates a plot with all variables for each station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1032e1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each file, import to a pandas dataframe and make a plot\n",
    "for file in files:\n",
    "    # define file name\n",
    "    xls = './qualar-data/'+file\n",
    "    # read to a pandas dataframe\n",
    "    df = pd.read_excel(xls)\n",
    "    \n",
    "    # create a list with column names of the dataframe \n",
    "    col_names = list(df.columns)\n",
    "    \n",
    "    # Convert the column with dates to a datetime format\n",
    "    df['Dates'] = pd.to_datetime(df[col_names[0]])\n",
    "\n",
    "    # plot all variables, setting dates as the index\n",
    "    df.set_index('Dates').plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
