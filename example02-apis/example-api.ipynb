{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d442f150",
   "metadata": {},
   "source": [
    "### GreenDS\n",
    "\n",
    "# Fundamentals of Agro-Environmental Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94780fb",
   "metadata": {},
   "source": [
    "## Example APIs and Web scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26b0967",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "The purpose of this Jupyter Notebook exercise is to demonstrate the methods available to obtain data from online services. Two examples are explored:\n",
    "- web data services based on REST APIs\n",
    "- web scraping from online web pages\n",
    "\n",
    "Sometime, web pages use APIs to expose information and services, but no documentation is provided. We will learn how to identify the existence of these services, to use them in more efficient data collection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e1dd97",
   "metadata": {},
   "source": [
    "## Web scraping Air Quality data\n",
    "\n",
    "The QUALAR (https://qualar.apambiente.pt/) is a web platform of APA, the Portuguese Environment Agency, that displays online air quality data sampled by on air monitoring stations in Portugal. Unfortunately, the platform does not expose to the final user, or provides documentation how to use the API service that was implemented for web users downloads. Downloads are generated as XLSX files.\n",
    "\n",
    "However, it is possible to hack the source code of the webpage to identify that there is an implemented API, and that it can be used to facilite efficient download of data. This exercise will demonstrate that, with the following steps:\n",
    "- check how are downloads generated from the website\n",
    "- identify and collect the parameters that define the data download\n",
    "- use the API to download data\n",
    "- as a bonus, visualise a timeseries of a data quality variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c965a4da",
   "metadata": {},
   "source": [
    "## Prepare your environment\n",
    "\n",
    "1. Create two directories inside your projects' directory named `raw-data` and `qualar-data`, with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403626d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir raw-data qualar-data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ca1cb8",
   "metadata": {},
   "source": [
    "## 1. Data download for human users\n",
    "\n",
    "Visit the data download page of QUALAR, at https://qualar.apambiente.pt/downloads. It displays a table with a list of Air Monitoring Stations, with the following columns:\n",
    "- Region (Região)\n",
    "- Municipality (Concelho)\n",
    "- Station (Estação)\n",
    "- Station type (Tipo de Estação), with categories traffic, industrial and background\n",
    "- Área type (Tipo de Área), with categories urban, suburban and rural\n",
    "- columns for the following pollutants: O3, NO2, CO, SO2, PM10, PM2.5, C6H6, other\n",
    "\n",
    "On the top, the page has two fields to define the time range for the data download, and on the left several buttons to activate filters about the type of station and type of área.\n",
    "\n",
    "To make a download, users can click on arrows that are available for each station and each pollutant, or if they want to download all pollutants for a station, they can click directly on the station name. After clicking, the download file is generated for the requested options as a excel file (xlsx).\n",
    "\n",
    "*Try to make a download in this way, and check the file downloaded.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5ced8c",
   "metadata": {},
   "source": [
    "## 2. Verify how are downloads generated by inspecting the webpage source code\n",
    "\n",
    "It is possible to inspect the source code of the table, and the behaviour of the page when a download is solicited. Checking this we can try to identify which methods are used to provide data to users. If we manage to verify that the web page is served by an API, and we can identify which parameters define a request, then it would be possible to generate a script to speed up downloads.\n",
    "\n",
    "**1. Activate the Inspect Tool of the source code of the webpage.**\n",
    "\n",
    "*Open in your web browser, navigate to https://qualar.apambiente.pt/downloads.Afterwards, in the menu of your browser, find the option **Web Developer Tools** or **Developer Tools** (in Firefox or Chrome, you will find it in **More tools**). This will open a new panel in the browser.*\n",
    "\n",
    "**2. Check the method to generate downloads**\n",
    "\n",
    "As mentioned before, clicking on the name of a station will generate a download with all data for that station. This means that through the HTTP protocol, a request is made through the network. Checking which request was made (which URL request was send) is a good way of verifying what was the information send to the web server.\n",
    "\n",
    "*On the Developer Panel, click on the tab **Netwotk**. After that, click on the name of a station to make a download request. This will generate a new row on the panel, with the information about a **GET** request.*\n",
    "\n",
    "One of the parameters in that row is the name or file field, which shows the URL sent to the server, e.g.:\n",
    "\n",
    "```https://qualar.apambiente.pt/api/download.php?poluente_id=0&estacao_id=3082&data_inicio=2021-01-01&data_fim=2021-12-31&influencias=1,2,3&ambientes=1,2,4```\n",
    "\n",
    "We can identify the following sections in the URL:\n",
    "\n",
    "Host URL: ```https://qualar.apambiente.pt/api/download.php```\n",
    "\n",
    "Parameters:```poluente_id=0&estacao_id=3082&data_inicio=2021-01-01&data_fim=2021-12-31&influencias=1,2,3&ambientes=1,2,4```\n",
    "\n",
    "The meaning of the parameters is more or less obvious:\n",
    "\n",
    "`poluente_id` - the ID of the pollutant. The value zero should mean all pollutants\n",
    "\n",
    "`estacao_ID` - the ID of the station\n",
    "\n",
    "`data_inicio` - starting date\n",
    "\n",
    "`data_fim` - ending date\n",
    "\n",
    "`influencias` - station type\n",
    "\n",
    "`ambientes` - area type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be9a9d6",
   "metadata": {},
   "source": [
    "**3. Verify that the method for download works**\n",
    "\n",
    "We have just identified an API service for downloading data. We can check if it works, testing with different parameters and see if results correspond to what is expected:\n",
    "\n",
    "*To download data only for **year 2020**, try the following modified URL:*\n",
    "\n",
    "```https://qualar.apambiente.pt/api/download.php?poluente_id=0&estacao_id=3082&data_inicio=2020-01-01&data_fim=2020-12-31&influencias=1,2,3&ambientes=1,2,4```\n",
    "\n",
    "The challenge now is to discover the values of the IDs of the air monitoring stations (the parameter `estacao_ID`). If we find these, we can make a script to make automatic requests to download the data files. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9a47c0",
   "metadata": {},
   "source": [
    "**4. Inspect the HTML source code** \n",
    "\n",
    "On the top left bar of the **Developer tools** panel, there is a arrow cursor option. Select this, and the place the mouse pointed on the name of one air monitoring station, in the table. You will verify that for each section of the web page where you hover your mouse, the corresponding HTML source code will be highlighted in the developer panel.\n",
    "\n",
    "**5. Select the section of the HTML code with the cell of the station name**\n",
    "\n",
    "Place the mouse so that the complete cell with the name of a station in the table is highlighted, and the click. In the source code, a line starting with the tag **td** should be selected.\n",
    "\n",
    "In the begining of that line, a triangle indicates that the inner HTLM code can be expanded. Remember that HTML is a hierarchical language, where html tags placed inside other tags are \"child\" or \"depended\" of these.\n",
    "\n",
    "The html line looks, for example, like the following:\n",
    "\n",
    "```html\n",
    "<td style=\"background-color: #EBF7FF; text-align: center; vertical-align: middle; \">\n",
    "    <label title=\"Dados de todos os poluentes para uma estação num dado ano\" \n",
    "           style=\"color:#0000ff; cursor:pointer\" onclick=\"tableDataManager.openExcel(3082)\">\n",
    "        <u><b>Alfragide/Amadora</b></u>\n",
    "    </label>\n",
    "</td>\n",
    "```\n",
    "\n",
    "The interesting about that html is that the **onclick** event on the **label** tag is an actionable event that triggers a method to open an Excel, **with an ID 3082**. This was for the station Alfragide/Amadora. If we try another station, the ID will be different. We have, therefore, found a way of identifying the IDs of all air monitoring stations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f5d443",
   "metadata": {},
   "source": [
    "## 3. Scrap the HTML source code to obtain IDs of the stations\n",
    "\n",
    "We will scrap the html of table in https://qualar.apambiente.pt/downloads to obtain the IDs of the air monitoring stations. For that, we will use the phyton module **BeautifulSoup**.\n",
    "\n",
    "**1. Get the table content**\n",
    "\n",
    "In the web page, the table content is loaded dynamically. It is possible to use python libraries to obtain that html code, but it is easier to do it manually.\n",
    "- go to the Inspector or Elements panel of the Developer Tool in your browser, depending of which browser you use\n",
    "- navigate down through the html code hierarchy until you find the html tag `<tbody>`. Verify in the web page that the correct section of the table containing data is highlighted.\n",
    "- right-click on top of the `<tbody>` tag, and select Copy --> Copy Outer HTML\n",
    "- paste the memory context to a new file in your text editor (VS Code, Notepad++, etc), and save it with the name *qualar_table.html* in the `raw-data` o your project.\n",
    "\n",
    "**2. Install and import** *BeautifulSoup*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39acd925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't have BeautifulSoup library installed, you can do it at the shell terminal\n",
    "# with the following commands:\n",
    "#\n",
    "# $ pip3 install BeautifulSoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af1829f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import BeautifulSoup and open the html file\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d8b9d0",
   "metadata": {},
   "source": [
    "**3. Get air pollution station IDs**\n",
    "\n",
    "Extract the station ID from the html text using BeautifulSoup. We will store the list of station IDs in a list to use latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d7d58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the html file\n",
    "with open(\"./raw-data/qualar_table.html\") as html_table:\n",
    "    soup = BeautifulSoup(html_table, 'html.parser')\n",
    "\n",
    "# create a list to store ids\n",
    "station_ids = []\n",
    "\n",
    "\n",
    "# get ids from html\n",
    "for row in soup.find_all('label'):\n",
    "    row_text = row['onclick']\n",
    "    ids = row_text[row_text.find( '(' )+1:row_text.find( ')' )]\n",
    "    station_ids.append(ids)\n",
    "    \n",
    "print(station_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2041e67b",
   "metadata": {},
   "source": [
    "## 4. Download files using the API\n",
    "\n",
    "We can now make requests to the API to download xlsx files. First, let's import the python modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fd6f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import requests\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8aefe4",
   "metadata": {},
   "source": [
    "After that, we will request to the REST API the excel files for each stattion, identified by its ID, and store them localy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8ebdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base URL of the service\n",
    "base_url = 'https://qualar.apambiente.pt/api/download.php'\n",
    "\n",
    "# Define params for the API request\n",
    "params = {\"poluente_id\":\"0\", \"influencias\":\"1,2,3\", \"ambientes\":\"1,2,4\"}\n",
    "\n",
    "# Define begining of the data period. You can change this for another year\n",
    "params[\"data_inicio\"] = \"2021-01-01\"\n",
    "\n",
    "# Define end of the data period. You can change this for another year\n",
    "params[\"data_fim\"] = \"2021-12-31\"\n",
    "\n",
    "# As this is not in production, we will replace the full list of ids by a smaller list of ids, to save time. \n",
    "# If you want all data, comment the next line\n",
    "station_ids = ['3095', '3103', '3104', '3096', '3070']\n",
    "\n",
    "# This iteration will go through the list of IDs, request the excel file based on the URL and save it locally.\n",
    "for id_value in station_ids:\n",
    "    # define name of file to be saved\n",
    "    filename = './qualar-data/station_'+ id_value + '.xlsx'\n",
    "    # add station_id to the parameters of the URL\n",
    "    params[\"estacao_id\"] = id_value\n",
    "    try:\n",
    "        r = requests.get(base_url, params)\n",
    "    except requests.exceptions.RequestException as e: \n",
    "        raise SystemExit(e)\n",
    "    print(r.url)\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd499dd",
   "metadata": {},
   "source": [
    "## 5. Preview dowloaded data\n",
    "\n",
    "We can import xlsx data to Pandas, and make a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a67bfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0421480",
   "metadata": {},
   "source": [
    "Let's print the list of available excel files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e45e7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list xlsx files in the data directory\n",
    "\n",
    "files = os.listdir('./qualar-data')\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b5e74e",
   "metadata": {},
   "source": [
    "The following imports the files to a Pandas dataframe, and creates a plot with all variables for each station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1032e1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each file, import to a pandas dataframe and make a plot\n",
    "for file in files:\n",
    "    # define file name\n",
    "    xls = './qualar-data/'+file\n",
    "    # read to a pandas dataframe\n",
    "    df = pd.read_excel(xls)\n",
    "    \n",
    "    # create a list with column names of the dataframe \n",
    "    col_names = list(df.columns)\n",
    "    \n",
    "    # Convert the column with dates to a datetime format\n",
    "    df['Dates'] = pd.to_datetime(df[col_names[0]])\n",
    "\n",
    "    # plot all variables, setting dates as the index\n",
    "    df.set_index('Dates').plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
