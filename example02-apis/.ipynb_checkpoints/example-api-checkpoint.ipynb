{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d442f150",
   "metadata": {},
   "source": [
    "### GreenDS\n",
    "\n",
    "# Fundamentals of Agro-Environmental Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94780fb",
   "metadata": {},
   "source": [
    "## Example APIs and Web scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26b0967",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "The purpose of this Jupyter Notebook exercise is to demonstrate the methods available to obtain data from online services. Two examples are explored:\n",
    "- web data services based on REST APIs\n",
    "- web scraping from online web pages\n",
    "\n",
    "Sometime, web pages use APIs to expose information and services, but no documentation is provided. We will learn how to identify the existence of these services, to use them in more efficient data collection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e1dd97",
   "metadata": {},
   "source": [
    "## Web scraping Air Quality data\n",
    "\n",
    "The QUALAR (https://qualar.apambiente.pt/) is a web platform of APA, the Portuguese Environment Agency, that displays online air quality data sampled by on air monitoring stations in Portugal. Unfortunately, the platform does not expose to the final user, or provides documentation how to use the API service that was implemented for web users downloads. Downloads are generated as XLSX files.\n",
    "\n",
    "However, it is possible to hack the source code of the webpage to identify that there is an implemented API, and that it can be used to facilite efficient download of data. This exercise will demonstrate that, with the following steps:\n",
    "- check how are downloads generated from the website\n",
    "- identify and collect the parameters that define the data download\n",
    "- use the API to download data\n",
    "- as a bonus, visualise a timeseries of a data quality variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ca1cb8",
   "metadata": {},
   "source": [
    "## 1. Data download for human users\n",
    "\n",
    "Visit the data download page of QUALAR, at https://qualar.apambiente.pt/downloads. It displays a table with a list of Air Monitoring Stations, with the following columns:\n",
    "- Region (Região)\n",
    "- Municipality (Concelho)\n",
    "- Station (Estação)\n",
    "- Station type (Tipo de Estação)\n",
    "- Área type (Tipo de Área)\n",
    "- columsn for the following pollutants: O3, NO2, CO, SO2, PM10, PM2.5, C6H6, other\n",
    "\n",
    "On the top, the page has two fields to define the time range for the data download, and on the left several buttons to activate filters about the type of station and type of área.\n",
    "\n",
    "To make a download, users can click on arrows that are available for each station and each pollutant, or if they want to download all pollutants for a station, they can click directly on the station name. After clicking, the download file is generated for the requested options as a excel file (xlsx).\n",
    "\n",
    "*Try to make a download in this way, and check the file downloaded.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fada99",
   "metadata": {},
   "source": [
    "## 2. Verify how are downloads generated inspecting the webpage source code\n",
    "\n",
    "It is possible to inspect the source code of the table, and the behaviour of the page when a download is solicited. Checking this we can try to identify which methods are used to provide data to users. If we manage to verify that the web page is served by an API, and we can identify which parameters define a request, then it would be possible to generate a script to speed up downloads.\n",
    "\n",
    "**1. Activate the Inspect Tool of the source code of the webpage.**\n",
    "\n",
    "*Open in your web browser, navigate to https://qualar.apambiente.pt/downloads.Afterwards, in the menu of your browser, find the option **Web Developer Tools** or **Developer Tools** (in Firefox or Chrome, you will find it in **More tools**). This will open a new panel in the browser.*\n",
    "\n",
    "**2. Check the method to generate downloads**\n",
    "\n",
    "As mentioned before, clicking on the name of a station will generate a download with all data for that station. This means that through the HTTP protocol, a request is made through the network. Checking which request was made (which URL request was send) is a good way of verifying what was the information send to the web server.\n",
    "\n",
    "*On the Developer Panel, click on the tab **Netwotk**. After that, click on the name of a station to make a download request. This will generate a new row on the panel, with the information about a **GET** request.*\n",
    "\n",
    "One of the parameters in that row is the name or file field, which shows the URL sent to the server, e.g.:\n",
    "\n",
    "```https://qualar.apambiente.pt/api/download.php?poluente_id=0&estacao_id=1041&data_inicio=2021-01-01&data_fim=2021-12-31&influencias=1,2,3&ambientes=1,2,4```\n",
    "\n",
    "We can identify the following sections in the URL:\n",
    "\n",
    "Host URL: ```https://qualar.apambiente.pt/api/download.php```\n",
    "\n",
    "Parameters:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4ac485",
   "metadata": {},
   "source": [
    "On the top left bar of this panel, there is a arrow cursor otion. Select this, and the place the mouse pointed on the name of one air monitoring station. You will verify that for each section of the web page where you hover your mouse, the corresponding HTML source code will be highlighted in the developer panel.\n",
    "\n",
    "**2. Select a section of the HTML code**\n",
    "\n",
    "Place the mouse so that the complete cell with the name of a station in the table is highlighted, and click. In the source code, a line starting with the tag **td** should be selected.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39acd925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't have pandas library installed, you can do it at the shell terminal\n",
    "# with the following commands:\n",
    "#\n",
    "# $ pip3 install pandas\n",
    "# $ pip3 install sklearn\n",
    "# $ pip3 install seaborn\n",
    "# $ pip3 install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fd6f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas library\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e45c1d",
   "metadata": {},
   "source": [
    "## 2. Download the data file from Kaggle\n",
    "Go to http://www.kaggle.com/uciml/breast-cancer-wisconsin-data and download the `data.csv` file. Place the file at the `raw-data` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff89ade",
   "metadata": {},
   "source": [
    "## 3. Read and preview data\n",
    "Read the data file, and print the shape and a preview of the table:\n",
    "- number of rows\n",
    "- number of properties (columns)\n",
    "- show the first two rows of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70285455",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cancer_data = pd.read_csv('./raw-data/data.csv')\n",
    "pd.options.display.max_columns = len(cancer_data)\n",
    "print(f'Number of entries: {cancer_data.shape[0]:,}\\n'\n",
    "      f'Number of features: {cancer_data.shape[1]:,}\\n\\n'\n",
    "      f'Number of missing values: {cancer_data.isnull().sum().sum()}\\n\\n'\n",
    "      f'{cancer_data.head(2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f30a4d6",
   "metadata": {},
   "source": [
    "The table was loaded to `pandas`, which has the possibility to show a preview of data (head):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4aa62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14552508",
   "metadata": {},
   "source": [
    "## 4. Clean and explore data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2a2100",
   "metadata": {},
   "source": [
    "You can scroll to the last column of the table above and verify that it contains no values (NaN). We need to remove the last column with missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625e3680",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_data = cancer_data.drop('Unnamed: 32', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ea82b6",
   "metadata": {},
   "source": [
    "It is possible to calculate descriptive statistics parameters of the attributes of the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21fea3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dd3b60",
   "metadata": {},
   "source": [
    "Next, let's calculate how many women have a confirmed cancer (a malignant breast tumor)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8177bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_data['diagnosis'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef669182",
   "metadata": {},
   "source": [
    "We can calculate these values as percentages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfcb2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(cancer_data['diagnosis'].value_counts()*100/len(cancer_data)).convert_dtypes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1acac8",
   "metadata": {},
   "source": [
    "## 5. Visualize data\n",
    "\n",
    "We can get a better insight of the data if we compare values for benign and malignant cases. Seaborn is one of the powerfull libraries to visualize data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c1f1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set(style=\"ticks\", color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384de985",
   "metadata": {},
   "outputs": [],
   "source": [
    "radius = cancer_data[['radius_mean','radius_se','radius_worst','diagnosis']]\n",
    "sns.pairplot(radius, hue='diagnosis',palette=\"husl\", markers=[\"o\", \"s\"],height=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03462eb",
   "metadata": {},
   "source": [
    "We can do another visualization, adding linear regression lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f03a0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "texture = cancer_data[['texture_mean','texture_se','texture_worst','diagnosis']]\n",
    "sns.pairplot(texture, hue='diagnosis', palette=\"husl\",height=4, kind=\"reg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a6799e",
   "metadata": {},
   "source": [
    "Another visualization which display the histogram for each category, is called violinplot. We will do this in groups of ten variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557ba6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y includes our labels and x includes our features\n",
    "y = cancer_data.diagnosis # M or B \n",
    "list_drp = ['id','diagnosis']\n",
    "x = cancer_data.drop(list_drp, axis = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bd95e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_dia = y\n",
    "data = x\n",
    "# standardization of the data\n",
    "data_n_2 = (data - data.mean()) / (data.std())\n",
    "data = pd.concat([y,data_n_2.iloc[:,0:30]],axis=1)\n",
    "data = pd.melt(data,id_vars=\"diagnosis\",\n",
    "                    var_name=\"features\",\n",
    "                    value_name='value')\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data,split=True, inner=\"quart\",palette ={\"B\": \"g\", \"M\": \"r\"})\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3101b24f",
   "metadata": {},
   "source": [
    "We can represent using box plots the worst values of the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a5f367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# box-plots\n",
    "data = pd.concat([y,data_n_2.iloc[:,20:30]],axis=1)\n",
    "data = pd.melt(data,id_vars=\"diagnosis\",\n",
    "                    var_name=\"features\",\n",
    "                    value_name='value')\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.boxplot(x='features', y='value', hue='diagnosis', data=data, palette ={\"B\": \"g\", \"M\": \"r\"})\n",
    "plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0afa27",
   "metadata": {},
   "source": [
    "To explore correlations between independent variables, we can calculate the correlation matrix, and represented with a heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6070ab1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation map\n",
    "f,ax = plt.subplots(figsize=(18, 18))\n",
    "sns.heatmap(x.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6924b714",
   "metadata": {},
   "source": [
    "## 6. Build the model\n",
    "\n",
    "We will calculate two models: on based on the [K-nearest neighors (KNN)](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) algorithm, and the other based on [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression).\n",
    "\n",
    "First, we will define the X (independent) and Y (dependent) variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4036bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cancer_data.iloc[:, 2:32].values\n",
    "y = cancer_data.iloc[:, 1].values\n",
    "\n",
    "# Encoding categorical data\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "labelencoder_y = LabelEncoder()\n",
    "y = labelencoder_y.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6152fd0",
   "metadata": {},
   "source": [
    "It is important to divide the dataset in two subsets, one for training (creating) the model, and other for testing. This is important to make sure that the model is not overfitted, and can be applied to other data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a43b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ddd2ff",
   "metadata": {},
   "source": [
    "In this example, we will try two models:\n",
    "- K-Nearest Neighbor (KNN)\n",
    "- Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3319c0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# KNN\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "knn_predictions = knn.predict(X_test)\n",
    "\n",
    "# Logistic regression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr_predictions = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0b04b0",
   "metadata": {},
   "source": [
    "## 7. Evaluate the model\n",
    "\n",
    "We can calculate the accuracy of the models. This value returns the fraction of correctly classified samples, in the test subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75564eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "print(f'Accuracy scores:\\n'\n",
    "      f'KNN model:\\t\\t   {accuracy_score(y_test, knn_predictions):.3f}\\n'\n",
    "      f'Logistic regression model: {accuracy_score(y_test, lr_predictions):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2769dd",
   "metadata": {},
   "source": [
    "Another way of evaluation the model is to calculate the confusion matrix *C*, in which *C<sub>i,j</sub>* is the number of observations which true value is *i*, and was predicted to be *j*.\n",
    "It gives the values of true negatives (*C<sub>0,0</sub>*), false negatives (*C<sub>1,0</sub>*), true positives (*C<sub>1,1</sub>*) and false positives (*C<sub>0,1</sub>*). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19321635",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = confusion_matrix(knn_predictions, y_test)\n",
    "sns.heatmap(matrix, cbar=False, annot=True)\n",
    "plt.xlabel('Predict')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix - Logistic Regression model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
