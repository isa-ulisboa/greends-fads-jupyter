{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aba5178",
   "metadata": {},
   "source": [
    "### GreenDS\n",
    "# Fundamentals of Data Science\n",
    "## Example on Unsupervised Machine Learning - Clustering\n",
    "### Example 03.2\n",
    "\n",
    "### Introduction\n",
    "\n",
    "This Jupyter Notebook continues on the exeample 03.1. In this case, we will perform a dimensionality reduction using the Principal Component Analysis (PCA), and afterwards, will repeat the cluster analysis. The goal is to improve the quality of the cluster definition.\n",
    "\n",
    "The data to be used is from the Agricultural Census of Portugal, from which data on **level of education**, **labour** data and **production** from 2019 was aggregated in one table, for the level of freguesia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72728b18",
   "metadata": {},
   "source": [
    "## 1. Prepare your environment and explore data\n",
    "\n",
    "The first steps will be the same of the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d4e06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef4f9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "census_df = pd.read_csv(\"./raw-data/data_agric_census_freg.csv\")\n",
    "census_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abcb646",
   "metadata": {},
   "outputs": [],
   "source": [
    "census_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06c8827",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='white',font_scale=1.3, rc={'figure.figsize':(20,20)})\n",
    "ax=census_df.hist(bins=20,color='red' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d641b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "census_df.plot( kind = 'box', subplots = True, layout = (4,4), sharex = False, sharey = False,color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e405085",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = ['Norte','Centro','√Årea Metropolitana de Lisboa', 'Alentejo', 'Algarve']\n",
    "df1 = census_df.loc[census_df['NUTS2'].isin(values)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e2a095",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4712e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.drop(['municipality', 'freguesia', 'NUTS2'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0969e01e",
   "metadata": {},
   "source": [
    "## 2. Perform the Principal Component Analysis\n",
    "\n",
    "Data needs to be scaled, otherwise variables with higher absolute value would have a larger weight, generating bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfcf067",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "data_cluster=df1.copy()\n",
    "data_cluster[data_cluster.columns]=std_scaler.fit_transform(data_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e52b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cluster.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46515ec",
   "metadata": {},
   "source": [
    "Here we calculate the PCA, in this case retaining only two components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d020aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca_2 = PCA(2)\n",
    "pca_2_result = pca_2.fit_transform(data_cluster)\n",
    "\n",
    "print ('Cumulative variance explained by 2 principal components: {:.2%}'.format(np.sum(pca_2.explained_variance_ratio_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9b005f",
   "metadata": {},
   "source": [
    "The comulative value of variance retained is around 70% of the total variance, which can be considered a good value. However, we should check the plot of the first factorial plane, which used the two principal components where samples were projected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7835304",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='white', rc={'figure.figsize':(9,6)},font_scale=1.1)\n",
    "\n",
    "plt.scatter(x=pca_2_result[:, 0], y=pca_2_result[:, 1], color='red',lw=0.1)\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('Data represented by the 2 strongest principal components',fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac90fc05",
   "metadata": {},
   "source": [
    "We can se that the plot is highly depedent on a single sample, which generates high bias in the results. We should consider to clean the sample set from outliers. We will ise Isolation Forest to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5846cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "df2 = df1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f817dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model building\n",
    "model=IsolationForest(n_estimators=150, max_samples='auto', contamination=float(0.1), max_features=1.0)\n",
    "model.fit(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7b2ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding 'scores' and 'anomaly' colums to df\n",
    "scores=model.decision_function(df2)\n",
    "anomaly=model.predict(df2)\n",
    "\n",
    "df2['scores']=scores\n",
    "df2['anomaly']=anomaly\n",
    "\n",
    "anomaly = df2.loc[df2['anomaly']==-1]\n",
    "anomaly_index = list(anomaly.index)\n",
    "print('Total number of outliers is:', len(anomaly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0972a941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping outliers\n",
    "df2 = df2.drop(anomaly_index, axis = 0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc950580",
   "metadata": {},
   "source": [
    "We can repeat the data visualisation to see the effects of removing the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695d9dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='white',font_scale=1.3, rc={'figure.figsize':(20,20)})\n",
    "ax=df2.hist(bins=20,color='red' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e30a27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.plot( kind = 'box', subplots = True, layout = (4,4), sharex = False, sharey = False,color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be7a579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping columns that we don't need any more\n",
    "df2.drop(['scores', 'anomaly'], axis = 1, inplace =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8247a970",
   "metadata": {},
   "source": [
    "## Repeat PCA calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca62bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "data_cluster=df2.copy()\n",
    "data_cluster[data_cluster.columns]=std_scaler.fit_transform(data_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f511a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca_2 = PCA(2)\n",
    "pca_2_result = pca_2.fit_transform(data_cluster)\n",
    "\n",
    "print ('Cumulative variance explained by 2 principal components: {:.2%}'.format(np.sum(pca_2.explained_variance_ratio_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6fe40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='white', rc={'figure.figsize':(9,6)},font_scale=1.1)\n",
    "\n",
    "plt.scatter(x=pca_2_result[:, 0], y=pca_2_result[:, 1], color='red',lw=0.1)\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('Data represented by the 2 strongest principal components',fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47df3e81",
   "metadata": {},
   "source": [
    "This time, the bias is not as pronounced as before. We will proceed to the cluster creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add650a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_2_result = pd.DataFrame(pca_2_result, columns=[\"PC1\",\"PC2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a692ca2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "wcss = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n",
    "    kmeans.fit(pca_2_result)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.title('The Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6731c8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as sch\n",
    "from matplotlib import pyplot\n",
    "pyplot.figure(figsize=(12, 5))\n",
    "dendrogram = sch.dendrogram(sch.linkage(pca_2_result, method = 'ward'))\n",
    "plt.title('Dendrogram')\n",
    "plt.ylabel('Euclidean distances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042c42fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kmeans = pca_2_result.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7871625c",
   "metadata": {},
   "source": [
    "We will use four clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11df3cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model\n",
    "kmeans = KMeans(n_clusters = 4, init = 'k-means++', random_state = 42)\n",
    "y_kmeans = kmeans.fit_predict(df_kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c422d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5405b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We called the df, that's why we need to refer to previous df to add cluster numbers\n",
    "df_kmeans = pca_2_result.copy()\n",
    "# Checking number of items in clusters and creating 'Cluster' column\n",
    "df_kmeans['Cluster'] = y_kmeans\n",
    "df_kmeans['Cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f659018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(15,7))\n",
    "sns.scatterplot(data=df_kmeans, x='PC1', y='PC2', hue = 'Cluster', s=15, palette=\"tab10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7229a1",
   "metadata": {},
   "source": [
    "# Hierarchical clustering\n",
    "## Agglomerative clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de0776f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying data sets\n",
    "df_AgglomerativeC = pca_2_result.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb449e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Training model\n",
    "AgglomerativeC = AgglomerativeClustering(n_clusters=4, metric = 'euclidean', linkage = 'ward')\n",
    "y_AgglomerativeC = AgglomerativeC.fit_predict(df_AgglomerativeC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cf7f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We called the df, that's why we need to refer to previous df to add cluster numbers\n",
    "df_AgglomerativeC = pca_2_result.copy()\n",
    "# Checking number of items in clusters and creating 'Cluster' column\n",
    "df_AgglomerativeC['Cluster'] = y_AgglomerativeC\n",
    "df_AgglomerativeC['Cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113758be",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "sns.scatterplot(data=df_AgglomerativeC, x='PC1', y='PC2', hue = 'Cluster', s=15, palette=\"tab10\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
